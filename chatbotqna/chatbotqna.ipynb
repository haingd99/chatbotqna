{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Disbale all warning messages\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data from documents (PDF)\n",
    "DATA_PATH = 'D:/Data/AIO/Chatbot QnA/chatbotqna/data/YOLOv10_Tutorials.pdf'\n",
    "loader = PyPDFLoader(DATA_PATH)\n",
    "# loader = Loader(DATA_PATH)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number pf chunks: 33\n",
      "Print first doc: page_content='AI VIET NAM – AI COURSE 2024\n",
      "Tutorial: Phát hiện đối tượng trong ảnh với\n",
      "YOLOv10\n",
      "Dinh-Thang Duong, Nguyen-Thuan Duong, Minh-Duc Bui và\n",
      "Quang-Vinh Dinh\n",
      "Ngày 20 tháng 6 năm 2024\n",
      "I. Giới thiệu\n",
      "Object Detection (Tạm dịch: Phát hiện đối tượng) là một bài toán cổ điển thuộc lĩnh vực\n",
      "Computer Vision. Mục tiêu của bài toán này là tự động xác định vị trí của các đối tượng trong\n",
      "một tấm ảnh. Tính tới thời điểm hiện tại, đã có rất nhiều phương pháp được phát triển nhằm\n",
      "giải quyết hiệu quả bài toán này. Trong đó, các phương pháp thuộc họ YOLO (You Only Look\n",
      "Once) thu hút được sự chú ý rất lớn từ cộng đồng nghiên cứu bởi độ chính xác và tốc độ thực\n",
      "thi mà loại mô hình này mang lại.\n",
      "Hình 1: Logo của mô hình YOLO. Ảnh: link.\n",
      "Thời gian vừa qua, Ao Wang và các cộng sự tại Đại học Thanh Hoa (Tsinghua University)\n",
      "đã đề xuất mô hình YOLOv10 trong bài báo YOLOv10: Real-Time End-to-End Object\n",
      "Detection [10]. Với những cải tiến mới, mô hình đã đạt được hiệu suất vượt trội hơn so với các' metadata={'source': 'D:/Data/AIO/Chatbot QnA/chatbotqna/data/YOLOv10_Tutorials.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "#Init text split\n",
    "\n",
    "txt_spliter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "txt_docs = txt_spliter.split_documents(documents)\n",
    "\n",
    "print(f\"Number pf chunks: {len(txt_docs)}\")\n",
    "print(f\"Print first doc: {txt_docs[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Vetorization:\n",
    "embeding = HuggingFaceEmbeddings()\n",
    "vector_db = Chroma.from_documents(documents=txt_docs, embedding=embeding)\n",
    "retriver = vector_db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Testing conversation\n",
    "\n",
    "results = retriver.invoke(\"What is YOLOv10?\")\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d121dd68064749ba68dcf92a25f0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build LLM\n",
    "#Initial cfg for LLM\n",
    "nf4_cfg = BitsAndBytesConfig(load_in_4bit=True,\\\n",
    "                            bnb_4bit_quant_type=\"nf4\", \\\n",
    "                            bnb_4bit_use_double_quant=True,\\\n",
    "                            bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "MODEL_NAME = \"lmsys/vicuna-7b-v1.5\"\n",
    "\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config = nf4_cfg, low_cpu_mem_usage=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "llm_model_pipline = pipeline(\"text-generation\",\\\n",
    "                            model=llm_model,\\\n",
    "                            tokenizer=tokenizer,\\\n",
    "                            max_new_tokens=512,\\\n",
    "                            pad_token_id = tokenizer.eos_token_id,\\\n",
    "                            device_map=\"auto\")#{\"\": \"cuda:0\"})\n",
    "\n",
    "llm_hugg_pipline = HuggingFacePipeline(pipeline=llm_model_pipline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriver | format_docs, \"question\": RunnablePassthrough()}\n",
    "    |prompt\n",
    "    |llm_hugg_pipline\n",
    "    |StrOutputParser()\n",
    ")\n",
    "\n",
    "# USER_QUESTION = \"YOLOv10 là gì?\"\n",
    "# output = rag_chain.invoke(USER_QUESTION)\n",
    "# answer = output.split(\"Answer:\")[1].strip()\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv10 là một phiên bản của mô hình dự đoán hình ảnh YOLO (You Only Look Once) với tối ưu hóa tham số và độ trễ trong inference. Nó có mức độ chính xác tốt hơn so với các phiên bản trước và sử dụng PGI và GELAN để cải thiện hiệu suất và độ chính xác. Để sử dụng pre-trained models, các bạn cần tải về file trọng số (file.pt) và khởi tạo mô hình bằng cách sử dụng đoạn code của ultralytics. Sau đó, các bạn có thể tải ảnh cần dự đoán vào Colab tự động bằng cách sử dụng đoạn code của Google.\n"
     ]
    }
   ],
   "source": [
    "USER_QUESTION = \"YOLOv10 là gì? Cách sử dụng như thế nào? Cho vi du.\"\n",
    "output = rag_chain.invoke(USER_QUESTION)\n",
    "answer = output.split(\"Answer:\")[1].strip()\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
